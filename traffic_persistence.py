"""
Module de persistance SQLite pour l'historique du trafic Meshtastic.
Permet de conserver les donn√©es de trafic entre les red√©marrages du bot.
"""

import sqlite3
import json
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable
from collections import defaultdict, deque
import os
from utils import debug_print, info_print, error_print

logger = logging.getLogger(__name__)


class TrafficPersistence:
    """G√®re la persistance des donn√©es de trafic dans SQLite."""

    def __init__(self, db_path: str = "traffic_history.db", error_callback: Optional[Callable[[Exception, str], None]] = None):
        """
        Initialise la connexion √† la base de donn√©es.

        Args:
            db_path: Chemin vers le fichier de base de donn√©es SQLite
            error_callback: Fonction √† appeler en cas d'erreur d'√©criture (optionnel)
                           Signature: error_callback(error: Exception, operation: str)
        """
        self.db_path = db_path
        self.conn = None
        self.error_callback = error_callback
        self._init_database()

    def _init_database(self):
        """Initialise la base de donn√©es et cr√©e les tables si n√©cessaire."""
        try:
            # V√©rifier si le fichier DB existe et est vide (corrompu)
            if os.path.exists(self.db_path):
                file_size = os.path.getsize(self.db_path)
                if file_size == 0:
                    logger.warning(f"‚ö†Ô∏è Base de donn√©es vide/corrompue d√©tect√©e ({self.db_path}), suppression...")
                    os.remove(self.db_path)

            self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
            self.conn.row_factory = sqlite3.Row

            # V√©rifier l'int√©grit√© de la base de donn√©es
            cursor = self.conn.cursor()
            try:
                cursor.execute("PRAGMA integrity_check")
                integrity = cursor.fetchone()[0]
                if integrity != 'ok':
                    logger.error(f"‚ùå Int√©grit√© DB compromise : {integrity}")
                    # Tentative de r√©paration ou recr√©ation
                    self.conn.close()
                    logger.info("Recr√©ation de la base de donn√©es...")
                    os.remove(self.db_path)
                    self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
                    self.conn.row_factory = sqlite3.Row
                    cursor = self.conn.cursor()
                else:
                    logger.info("‚úÖ Int√©grit√© de la DB v√©rifi√©e")
            except Exception as e:
                logger.warning(f"Impossible de v√©rifier l'int√©grit√© : {e}")

            cursor = self.conn.cursor()

            # Table pour tous les paquets
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS packets (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp REAL NOT NULL,
                    from_id TEXT NOT NULL,
                    to_id TEXT,
                    source TEXT,
                    sender_name TEXT,
                    packet_type TEXT NOT NULL,
                    message TEXT,
                    rssi INTEGER,
                    snr REAL,
                    hops INTEGER,
                    size INTEGER,
                    is_broadcast INTEGER,
                    is_encrypted INTEGER DEFAULT 0,
                    telemetry TEXT,
                    position TEXT,
                    hop_limit INTEGER,
                    hop_start INTEGER
                )
            ''')

            # Migration : ajouter is_encrypted si elle n'existe pas
            try:
                cursor.execute("SELECT is_encrypted FROM packets LIMIT 1")
            except sqlite3.OperationalError:
                # La colonne n'existe pas, l'ajouter
                logger.info("Migration DB : ajout de la colonne is_encrypted")
                cursor.execute("ALTER TABLE packets ADD COLUMN is_encrypted INTEGER DEFAULT 0")
            
            # Migration : ajouter hop_limit si elle n'existe pas
            try:
                cursor.execute("SELECT hop_limit FROM packets LIMIT 1")
            except sqlite3.OperationalError:
                # La colonne n'existe pas, l'ajouter
                logger.info("Migration DB : ajout de la colonne hop_limit")
                cursor.execute("ALTER TABLE packets ADD COLUMN hop_limit INTEGER")
            
            # Migration : ajouter hop_start si elle n'existe pas
            try:
                cursor.execute("SELECT hop_start FROM packets LIMIT 1")
            except sqlite3.OperationalError:
                # La colonne n'existe pas, l'ajouter
                logger.info("Migration DB : ajout de la colonne hop_start")
                cursor.execute("ALTER TABLE packets ADD COLUMN hop_start INTEGER")

            # Index pour optimiser les requ√™tes sur les paquets
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_packets_timestamp
                ON packets(timestamp)
            ''')
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_packets_from_id
                ON packets(from_id)
            ''')
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_packets_type
                ON packets(packet_type)
            ''')

            # Table pour les messages publics
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS public_messages (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp REAL NOT NULL,
                    from_id TEXT NOT NULL,
                    sender_name TEXT,
                    message TEXT,
                    rssi INTEGER,
                    snr REAL,
                    message_length INTEGER,
                    source TEXT
                )
            ''')

            # Index pour optimiser les requ√™tes sur les messages
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_messages_timestamp
                ON public_messages(timestamp)
            ''')

            # Table pour les statistiques par n≈ìud
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS node_stats (
                    node_id TEXT PRIMARY KEY,
                    total_packets INTEGER,
                    total_bytes INTEGER,
                    packet_types TEXT,
                    hourly_activity TEXT,
                    message_stats TEXT,
                    telemetry_stats TEXT,
                    position_stats TEXT,
                    routing_stats TEXT,
                    last_updated REAL,
                    last_battery_level INTEGER,
                    last_battery_voltage REAL,
                    last_telemetry_update REAL,
                    last_temperature REAL,
                    last_humidity REAL,
                    last_pressure REAL,
                    last_air_quality REAL
                )
            ''')

            # Table pour les statistiques globales
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS global_stats (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    total_packets INTEGER,
                    total_bytes INTEGER,
                    packet_types TEXT,
                    unique_nodes TEXT,
                    last_reset REAL
                )
            ''')

            # Table pour les statistiques r√©seau
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS network_stats (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    total_hops INTEGER,
                    max_hops_seen INTEGER,
                    avg_rssi REAL,
                    avg_snr REAL,
                    packets_direct INTEGER,
                    packets_relayed INTEGER
                )
            ''')

            # Table pour le cache m√©t√©o (centralise tous les caches weather/rain/astro)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS weather_cache (
                    location TEXT NOT NULL,
                    cache_type TEXT NOT NULL,
                    data TEXT NOT NULL,
                    timestamp REAL NOT NULL,
                    PRIMARY KEY (location, cache_type)
                )
            ''')

            # Index pour nettoyage p√©riodique du cache expir√©
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_weather_cache_timestamp
                ON weather_cache(timestamp)
            ''')

            # Table pour les informations de voisinage (neighbor info)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS neighbors (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp REAL NOT NULL,
                    node_id TEXT NOT NULL,
                    neighbor_id TEXT NOT NULL,
                    snr REAL,
                    last_rx_time INTEGER,
                    node_broadcast_interval INTEGER,
                    source TEXT DEFAULT 'radio'
                )
            ''')
            
            # Migration : ajouter source si elle n'existe pas
            try:
                cursor.execute("SELECT source FROM neighbors LIMIT 1")
            except sqlite3.OperationalError:
                # La colonne n'existe pas, l'ajouter
                logger.info("Migration DB : ajout de la colonne source aux neighbors")
                cursor.execute("ALTER TABLE neighbors ADD COLUMN source TEXT DEFAULT 'radio'")

            # Index pour optimiser les requ√™tes sur les voisins
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_neighbors_timestamp
                ON neighbors(timestamp)
            ''')
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_neighbors_node_id
                ON neighbors(node_id)
            ''')
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_neighbors_composite
                ON neighbors(node_id, neighbor_id, timestamp)
            ''')

            # Migration : ajouter les colonnes de t√©l√©m√©trie √† node_stats si elles n'existent pas
            try:
                cursor.execute("SELECT last_battery_level FROM node_stats LIMIT 1")
            except sqlite3.OperationalError:
                # Les colonnes n'existent pas, les ajouter
                logger.info("Migration DB : ajout des colonnes de t√©l√©m√©trie √† node_stats")
                cursor.execute("ALTER TABLE node_stats ADD COLUMN last_battery_level INTEGER")
                cursor.execute("ALTER TABLE node_stats ADD COLUMN last_battery_voltage REAL")
                cursor.execute("ALTER TABLE node_stats ADD COLUMN last_telemetry_update REAL")
            
            # Migration : ajouter les colonnes d'environnement √† node_stats si elles n'existent pas
            try:
                cursor.execute("SELECT last_temperature FROM node_stats LIMIT 1")
            except sqlite3.OperationalError:
                # Les colonnes n'existent pas, les ajouter
                logger.info("Migration DB : ajout des colonnes d'environnement √† node_stats")
                cursor.execute("ALTER TABLE node_stats ADD COLUMN last_temperature REAL")
                cursor.execute("ALTER TABLE node_stats ADD COLUMN last_humidity REAL")
                cursor.execute("ALTER TABLE node_stats ADD COLUMN last_pressure REAL")
                cursor.execute("ALTER TABLE node_stats ADD COLUMN last_air_quality REAL")

            self.conn.commit()
            logger.info(f"‚úÖ Base de donn√©es initialis√©e : {self.db_path}")

            # V√©rifier que les tables sont bien cr√©√©es
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            logger.info(f"Tables cr√©√©es : {', '.join(tables)}")

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation de la base de donn√©es : {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def save_packet(self, packet: Dict[str, Any]):
        """
        Sauvegarde un paquet dans la base de donn√©es.

        Args:
            packet: Dictionnaire contenant les informations du paquet
        """
        try:
            # V√©rifier que la connexion est active
            if self.conn is None:
                logger.error("Connexion SQLite non initialis√©e, tentative de reconnexion")
                self._init_database()
                if self.conn is None:
                    logger.error("Impossible d'initialiser la connexion SQLite")
                    return

            cursor = self.conn.cursor()

            # Convertir les structures complexes en JSON
            telemetry = json.dumps(packet.get('telemetry')) if packet.get('telemetry') else None
            position = json.dumps(packet.get('position')) if packet.get('position') else None

            cursor.execute('''
                INSERT INTO packets (
                    timestamp, from_id, to_id, source, sender_name, packet_type,
                    message, rssi, snr, hops, size, is_broadcast, is_encrypted, telemetry, position,
                    hop_limit, hop_start
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                packet.get('timestamp'),
                packet.get('from_id'),
                packet.get('to_id'),
                packet.get('source'),
                packet.get('sender_name'),
                packet.get('packet_type'),
                packet.get('message'),
                packet.get('rssi'),
                packet.get('snr'),
                packet.get('hops'),
                packet.get('size'),
                1 if packet.get('is_broadcast') else 0,
                1 if packet.get('is_encrypted') else 0,
                telemetry,
                position,
                packet.get('hop_limit'),
                packet.get('hop_start')
            ))

            self.conn.commit()

            # Log p√©riodique pour suivre l'activit√© (tous les 50 paquets)
            if not hasattr(self, '_packet_count'):
                self._packet_count = 0
            self._packet_count += 1
            if self._packet_count % 50 == 0:
                logger.info(f"üì¶ {self._packet_count} paquets sauvegard√©s dans SQLite")

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la sauvegarde du paquet : {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # Notifier le callback d'erreur si configur√©
            if self.error_callback:
                try:
                    self.error_callback(e, 'save_packet')
                except Exception as cb_error:
                    logger.error(f"Erreur dans error_callback: {cb_error}")

    def save_public_message(self, message_data: Dict[str, Any]):
        """
        Sauvegarde un message public dans la base de donn√©es.

        Args:
            message_data: Dictionnaire contenant les informations du message
        """
        try:
            # V√©rifier que la connexion est active
            if self.conn is None:
                logger.error("Connexion SQLite non initialis√©e pour save_public_message")
                self._init_database()
                if self.conn is None:
                    return

            cursor = self.conn.cursor()

            cursor.execute('''
                INSERT INTO public_messages (
                    timestamp, from_id, sender_name, message, rssi, snr,
                    message_length, source
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                message_data.get('timestamp'),
                message_data.get('from_id'),
                message_data.get('sender_name'),
                message_data.get('message'),
                message_data.get('rssi'),
                message_data.get('snr'),
                message_data.get('message_length'),
                message_data.get('source')
            ))

            self.conn.commit()

        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde du message public : {e}")
            
            # Notifier le callback d'erreur si configur√©
            if self.error_callback:
                try:
                    self.error_callback(e, 'save_public_message')
                except Exception as cb_error:
                    logger.error(f"Erreur dans error_callback: {cb_error}")

    def save_node_stats(self, node_stats: Dict[str, Dict]):
        """
        Sauvegarde les statistiques par n≈ìud.

        Args:
            node_stats: Dictionnaire des statistiques par n≈ìud
        """
        try:
            cursor = self.conn.cursor()
            timestamp = datetime.now().timestamp()

            for node_id, stats in node_stats.items():
                # Extract telemetry stats for last battery data
                telemetry_stats = stats.get('telemetry_stats', {})
                last_battery = telemetry_stats.get('last_battery')
                last_voltage = telemetry_stats.get('last_voltage')
                last_temperature = telemetry_stats.get('last_temperature')
                last_humidity = telemetry_stats.get('last_humidity')
                last_pressure = telemetry_stats.get('last_pressure')
                last_air_quality = telemetry_stats.get('last_air_quality')
                
                # Determine telemetry update timestamp
                # Use current timestamp if we have fresh telemetry data, otherwise None
                has_telemetry = (last_battery is not None or last_voltage is not None or 
                                last_temperature is not None or last_humidity is not None or
                                last_pressure is not None or last_air_quality is not None)
                last_telemetry_update = timestamp if has_telemetry else None
                
                cursor.execute('''
                    INSERT OR REPLACE INTO node_stats (
                        node_id, total_packets, total_bytes, packet_types,
                        hourly_activity, message_stats, telemetry_stats,
                        position_stats, routing_stats, last_updated,
                        last_battery_level, last_battery_voltage, last_telemetry_update,
                        last_temperature, last_humidity, last_pressure, last_air_quality
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    node_id,
                    stats.get('total_packets', 0),
                    stats.get('total_bytes', 0),
                    json.dumps(dict(stats.get('by_type', {}))),
                    json.dumps(stats.get('hourly_activity', {})),
                    json.dumps(stats.get('message_stats', {})),
                    json.dumps(telemetry_stats),
                    json.dumps(stats.get('position_stats', {})),
                    json.dumps(stats.get('routing_stats', {})),
                    timestamp,
                    last_battery,
                    last_voltage,
                    last_telemetry_update,
                    last_temperature,
                    last_humidity,
                    last_pressure,
                    last_air_quality
                ))

            self.conn.commit()

        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde des statistiques par n≈ìud : {e}")

    def save_global_stats(self, global_stats: Dict):
        """
        Sauvegarde les statistiques globales.

        Args:
            global_stats: Dictionnaire des statistiques globales
        """
        try:
            cursor = self.conn.cursor()

            # Convertir le set en liste pour JSON
            unique_nodes = list(global_stats.get('unique_nodes', set()))

            cursor.execute('''
                INSERT OR REPLACE INTO global_stats (
                    id, total_packets, total_bytes, packet_types,
                    unique_nodes, last_reset
                ) VALUES (1, ?, ?, ?, ?, ?)
            ''', (
                global_stats.get('total_packets', 0),
                global_stats.get('total_bytes', 0),
                json.dumps(dict(global_stats.get('by_type', {}))),
                json.dumps(unique_nodes),
                global_stats.get('last_reset', datetime.now().timestamp())
            ))

            self.conn.commit()

        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde des statistiques globales : {e}")

    def save_network_stats(self, network_stats: Dict):
        """
        Sauvegarde les statistiques r√©seau.

        Args:
            network_stats: Dictionnaire des statistiques r√©seau
        """
        try:
            cursor = self.conn.cursor()

            cursor.execute('''
                INSERT OR REPLACE INTO network_stats (
                    id, total_hops, max_hops_seen, avg_rssi, avg_snr,
                    packets_direct, packets_relayed
                ) VALUES (1, ?, ?, ?, ?, ?, ?)
            ''', (
                network_stats.get('total_hops', 0),
                network_stats.get('max_hops_seen', 0),
                network_stats.get('avg_rssi', 0.0),
                network_stats.get('avg_snr', 0.0),
                network_stats.get('packets_direct', 0),
                network_stats.get('packets_relayed', 0)
            ))

            self.conn.commit()

        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde des statistiques r√©seau : {e}")

    def load_packets(self, hours: int = 24, limit: int = 5000) -> List[Dict]:
        """
        Charge les paquets depuis la base de donn√©es.

        Args:
            hours: Nombre d'heures √† charger
            limit: Nombre maximum de paquets √† charger

        Returns:
            Liste des paquets
        """
        try:
            cursor = self.conn.cursor()
            cutoff = (datetime.now() - timedelta(hours=hours)).timestamp()

            cursor.execute('''
                SELECT * FROM packets
                WHERE timestamp >= ?
                ORDER BY timestamp DESC
                LIMIT ?
            ''', (cutoff, limit))

            packets = []
            for row in cursor.fetchall():
                packet = dict(row)

                # Reconvertir les JSON en objets
                if packet.get('telemetry'):
                    packet['telemetry'] = json.loads(packet['telemetry'])
                if packet.get('position'):
                    packet['position'] = json.loads(packet['position'])

                packet['is_broadcast'] = bool(packet.get('is_broadcast'))

                packets.append(packet)

            return packets

        except Exception as e:
            logger.error(f"Erreur lors du chargement des paquets : {e}")
            return []

    def load_public_messages(self, hours: int = 24, limit: int = 2000) -> List[Dict]:
        """
        Charge les messages publics depuis la base de donn√©es.

        Args:
            hours: Nombre d'heures √† charger
            limit: Nombre maximum de messages √† charger

        Returns:
            Liste des messages
        """
        try:
            cursor = self.conn.cursor()
            cutoff = (datetime.now() - timedelta(hours=hours)).timestamp()

            cursor.execute('''
                SELECT * FROM public_messages
                WHERE timestamp >= ?
                ORDER BY timestamp DESC
                LIMIT ?
            ''', (cutoff, limit))

            messages = [dict(row) for row in cursor.fetchall()]
            return messages

        except Exception as e:
            logger.error(f"Erreur lors du chargement des messages publics : {e}")
            return []

    def load_node_stats(self) -> Dict[str, Dict]:
        """
        Charge les statistiques par n≈ìud.

        Returns:
            Dictionnaire des statistiques par n≈ìud
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute('SELECT * FROM node_stats')

            node_stats = {}
            for row in cursor.fetchall():
                node_id = row['node_id']
                
                # Load telemetry stats from JSON
                telemetry_stats = json.loads(row['telemetry_stats']) if row['telemetry_stats'] else {}
                
                # Add battery data from dedicated columns (takes precedence over JSON)
                if 'last_battery_level' in row.keys() and row['last_battery_level'] is not None:
                    telemetry_stats['last_battery'] = row['last_battery_level']
                if 'last_battery_voltage' in row.keys() and row['last_battery_voltage'] is not None:
                    telemetry_stats['last_voltage'] = row['last_battery_voltage']
                if 'last_telemetry_update' in row.keys() and row['last_telemetry_update'] is not None:
                    telemetry_stats['last_telemetry_update'] = row['last_telemetry_update']
                
                # Add environment data from dedicated columns
                if 'last_temperature' in row.keys() and row['last_temperature'] is not None:
                    telemetry_stats['last_temperature'] = row['last_temperature']
                if 'last_humidity' in row.keys() and row['last_humidity'] is not None:
                    telemetry_stats['last_humidity'] = row['last_humidity']
                if 'last_pressure' in row.keys() and row['last_pressure'] is not None:
                    telemetry_stats['last_pressure'] = row['last_pressure']
                if 'last_air_quality' in row.keys() and row['last_air_quality'] is not None:
                    telemetry_stats['last_air_quality'] = row['last_air_quality']
                
                node_stats[node_id] = {
                    'total_packets': row['total_packets'],
                    'total_bytes': row['total_bytes'],
                    'by_type': defaultdict(int, json.loads(row['packet_types']) if row['packet_types'] else {}),
                    'hourly_activity': defaultdict(int, json.loads(row['hourly_activity']) if row['hourly_activity'] else {}),
                    'message_stats': json.loads(row['message_stats']) if row['message_stats'] else {},
                    'telemetry_stats': telemetry_stats,
                    'position_stats': json.loads(row['position_stats']) if row['position_stats'] else {},
                    'routing_stats': json.loads(row['routing_stats']) if row['routing_stats'] else {}
                }

            return node_stats

        except Exception as e:
            logger.error(f"Erreur lors du chargement des statistiques par n≈ìud : {e}")
            return {}

    def load_global_stats(self) -> Optional[Dict]:
        """
        Charge les statistiques globales.

        Returns:
            Dictionnaire des statistiques globales ou None
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute('SELECT * FROM global_stats WHERE id = 1')
            row = cursor.fetchone()

            if row:
                unique_nodes = set(json.loads(row['unique_nodes']) if row['unique_nodes'] else [])

                return {
                    'total_packets': row['total_packets'],
                    'total_bytes': row['total_bytes'],
                    'by_type': defaultdict(int, json.loads(row['packet_types']) if row['packet_types'] else {}),
                    'unique_nodes': unique_nodes,
                    'last_reset': row['last_reset']
                }

            return None

        except Exception as e:
            logger.error(f"Erreur lors du chargement des statistiques globales : {e}")
            return None

    def load_network_stats(self) -> Optional[Dict]:
        """
        Charge les statistiques r√©seau.

        Returns:
            Dictionnaire des statistiques r√©seau ou None
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute('SELECT * FROM network_stats WHERE id = 1')
            row = cursor.fetchone()

            if row:
                return {
                    'total_hops': row['total_hops'],
                    'max_hops_seen': row['max_hops_seen'],
                    'avg_rssi': row['avg_rssi'],
                    'avg_snr': row['avg_snr'],
                    'packets_direct': row['packets_direct'],
                    'packets_relayed': row['packets_relayed']
                }

            return None

        except Exception as e:
            logger.error(f"Erreur lors du chargement des statistiques r√©seau : {e}")
            return None

    def save_neighbor_info(self, node_id: str, neighbors: List[Dict], source: str = 'radio'):
        """
        Sauvegarde les informations de voisinage pour un n≈ìud.

        Args:
            node_id: ID du n≈ìud (format !xxxxxxxx ou hex)
            neighbors: Liste des voisins avec leurs informations
            source: Source des donn√©es ('radio' ou 'mqtt')
        """
        try:
            if not neighbors:
                return

            cursor = self.conn.cursor()
            timestamp = time.time()

            for neighbor in neighbors:
                neighbor_id = neighbor.get('node_id')
                if not neighbor_id:
                    continue

                # Normaliser les IDs
                if isinstance(node_id, int):
                    node_id_str = f"!{node_id:08x}"
                else:
                    node_id_str = node_id if node_id.startswith('!') else f"!{node_id}"

                if isinstance(neighbor_id, int):
                    neighbor_id_str = f"!{neighbor_id:08x}"
                else:
                    neighbor_id_str = neighbor_id if neighbor_id.startswith('!') else f"!{neighbor_id}"

                cursor.execute('''
                    INSERT INTO neighbors (
                        timestamp, node_id, neighbor_id, snr,
                        last_rx_time, node_broadcast_interval, source
                    ) VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    timestamp,
                    node_id_str,
                    neighbor_id_str,
                    neighbor.get('snr'),
                    neighbor.get('last_rx_time'),
                    neighbor.get('node_broadcast_interval'),
                    source
                ))

            self.conn.commit()
            logger.debug(f"üíæ Sauvegarde {len(neighbors)} voisins pour {node_id_str} (source: {source})")

        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde des voisins : {e}")

    def load_neighbors(self, hours: int = 48) -> Dict[str, List[Dict]]:
        """
        Charge les informations de voisinage r√©centes.

        Args:
            hours: Nombre d'heures √† charger

        Returns:
            Dictionnaire {node_id: [liste de voisins]}
        """
        try:
            cursor = self.conn.cursor()
            cutoff = (datetime.now() - timedelta(hours=hours)).timestamp()

            # R√©cup√©rer les derni√®res entr√©es de voisinage par node/neighbor
            cursor.execute('''
                SELECT n1.*
                FROM neighbors n1
                INNER JOIN (
                    SELECT node_id, neighbor_id, MAX(timestamp) as max_timestamp
                    FROM neighbors
                    WHERE timestamp >= ?
                    GROUP BY node_id, neighbor_id
                ) n2 ON n1.node_id = n2.node_id
                    AND n1.neighbor_id = n2.neighbor_id
                    AND n1.timestamp = n2.max_timestamp
                ORDER BY n1.node_id, n1.timestamp DESC
            ''', (cutoff,))

            neighbors_by_node = defaultdict(list)
            for row in cursor.fetchall():
                node_id = row['node_id']
                neighbor_data = {
                    'node_id': row['neighbor_id'],
                    'snr': row['snr'],
                    'last_rx_time': row['last_rx_time'],
                    'node_broadcast_interval': row['node_broadcast_interval'],
                    'timestamp': row['timestamp'],
                    'source': row['source'] if 'source' in row.keys() else 'radio'  # Default for old data
                }
                neighbors_by_node[node_id].append(neighbor_data)

            return dict(neighbors_by_node)

        except Exception as e:
            logger.error(f"Erreur lors du chargement des voisins : {e}")
            return {}

    def cleanup_old_data(self, hours: int = 48):
        """
        Supprime les donn√©es plus anciennes que le nombre d'heures sp√©cifi√©.

        Args:
            hours: Nombre d'heures √† conserver
        """
        try:
            cursor = self.conn.cursor()
            cutoff = (datetime.now() - timedelta(hours=hours)).timestamp()

            cursor.execute('DELETE FROM packets WHERE timestamp < ?', (cutoff,))
            packets_deleted = cursor.rowcount
            
            cursor.execute('DELETE FROM public_messages WHERE timestamp < ?', (cutoff,))
            messages_deleted = cursor.rowcount
            
            cursor.execute('DELETE FROM neighbors WHERE timestamp < ?', (cutoff,))
            neighbors_deleted = cursor.rowcount

            self.conn.commit()

            logger.info(f"Nettoyage : {packets_deleted} paquets, {messages_deleted} messages, {neighbors_deleted} voisins supprim√©s (> {hours}h)")

            # Optimiser la base de donn√©es apr√®s le nettoyage
            cursor.execute('VACUUM')

        except Exception as e:
            logger.error(f"Erreur lors du nettoyage des anciennes donn√©es : {e}")

    def clear_all_data(self):
        """Efface toutes les donn√©es de trafic de la base de donn√©es."""
        try:
            cursor = self.conn.cursor()

            cursor.execute('DELETE FROM packets')
            cursor.execute('DELETE FROM public_messages')
            cursor.execute('DELETE FROM node_stats')
            cursor.execute('DELETE FROM global_stats')
            cursor.execute('DELETE FROM network_stats')
            cursor.execute('DELETE FROM neighbors')

            self.conn.commit()

            # Optimiser la base de donn√©es
            cursor.execute('VACUUM')

            logger.info("Toutes les donn√©es de trafic ont √©t√© effac√©es")

        except Exception as e:
            logger.error(f"Erreur lors de l'effacement des donn√©es : {e}")
            raise

    def get_stats_summary(self) -> Dict[str, Any]:
        """
        Retourne un r√©sum√© des statistiques de la base de donn√©es.

        Returns:
            Dictionnaire avec les statistiques de la base
        """
        try:
            cursor = self.conn.cursor()

            cursor.execute('SELECT COUNT(*) as count FROM packets')
            total_packets = cursor.fetchone()['count']

            cursor.execute('SELECT COUNT(*) as count FROM public_messages')
            total_messages = cursor.fetchone()['count']

            cursor.execute('SELECT COUNT(*) as count FROM node_stats')
            total_nodes = cursor.fetchone()['count']

            cursor.execute('SELECT COUNT(*) as count FROM neighbors')
            total_neighbors = cursor.fetchone()['count']

            cursor.execute('SELECT MIN(timestamp) as oldest, MAX(timestamp) as newest FROM packets')
            row = cursor.fetchone()
            oldest = row['oldest']
            newest = row['newest']

            # Taille du fichier de base de donn√©es
            db_size = os.path.getsize(self.db_path) if os.path.exists(self.db_path) else 0
            db_size_mb = db_size / (1024 * 1024)

            return {
                'total_packets': total_packets,
                'total_messages': total_messages,
                'total_nodes': total_nodes,
                'total_neighbors': total_neighbors,
                'oldest_packet': datetime.fromtimestamp(oldest).isoformat() if oldest else None,
                'newest_packet': datetime.fromtimestamp(newest).isoformat() if newest else None,
                'database_size_mb': round(db_size_mb, 2)
            }

        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des statistiques : {e}")
            return {}

    def get_weather_cache(self, location: str, cache_type: str, max_age_seconds: int = 300) -> Optional[str]:
        """
        R√©cup√®re les donn√©es m√©t√©o en cache si elles sont encore valides.

        Args:
            location: Nom de la localisation (ex: "Paris", "" pour default)
            cache_type: Type de cache ("weather", "rain", "astro")
            max_age_seconds: Dur√©e de validit√© du cache en secondes (d√©faut: 300 = 5min)

        Returns:
            Les donn√©es en cache si valides, None sinon
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
                SELECT data, timestamp FROM weather_cache
                WHERE location = ? AND cache_type = ?
            ''', (location, cache_type))

            row = cursor.fetchone()
            if row:
                data, timestamp = row['data'], row['timestamp']
                age = time.time() - timestamp

                if age < max_age_seconds:
                    logger.info(f"‚úÖ Cache m√©t√©o utilis√© ({cache_type}/{location}): {int(age)}s/{max_age_seconds}s")
                    return data
                else:
                    logger.info(f"‚è±Ô∏è Cache m√©t√©o expir√© ({cache_type}/{location}): {int(age)}s > {max_age_seconds}s")
                    return None

            return None

        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration du cache m√©t√©o : {e}")
            return None

    def get_weather_cache_with_age(self, location: str, cache_type: str, max_age_seconds: int = 86400) -> tuple:
        """
        R√©cup√®re les donn√©es m√©t√©o en cache avec leur √¢ge, m√™me si expir√©es.
        Utilis√© pour le pattern "serve stale first, refresh later".

        Args:
            location: Nom de la localisation (ex: "Paris", "" pour default)
            cache_type: Type de cache ("weather", "rain", "astro")
            max_age_seconds: √Çge maximum acceptable (d√©faut: 86400 = 24h)

        Returns:
            tuple: (data, age_hours) ou (None, 0) si pas de cache ou trop vieux
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
                SELECT data, timestamp FROM weather_cache
                WHERE location = ? AND cache_type = ?
            ''', (location, cache_type))

            row = cursor.fetchone()
            if row:
                data, timestamp = row['data'], row['timestamp']
                age_seconds = time.time() - timestamp
                age_hours = int(age_seconds / 3600)

                # Rejeter cache trop vieux
                if age_seconds > max_age_seconds:
                    logger.info(f"‚è∞ Cache trop vieux ({cache_type}/{location}): {age_hours}h > {max_age_seconds//3600}h")
                    return (None, 0)

                logger.info(f"üì¶ Cache r√©cup√©r√© ({cache_type}/{location}): √¢ge={age_hours}h")
                return (data, age_hours)

            return (None, 0)

        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration du cache m√©t√©o avec √¢ge : {e}")
            return (None, 0)

    def set_weather_cache(self, location: str, cache_type: str, data: str):
        """
        Stocke les donn√©es m√©t√©o en cache.

        Args:
            location: Nom de la localisation (ex: "Paris", "" pour default)
            cache_type: Type de cache ("weather", "rain", "astro")
            data: Donn√©es √† stocker (string)
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO weather_cache (location, cache_type, data, timestamp)
                VALUES (?, ?, ?, ?)
            ''', (location, cache_type, data, time.time()))

            self.conn.commit()
            logger.info(f"üíæ Cache m√©t√©o sauvegard√© ({cache_type}/{location})")

        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde du cache m√©t√©o : {e}")

    def cleanup_weather_cache(self, max_age_hours: int = 24):
        """
        Nettoie les entr√©es de cache m√©t√©o expir√©es.

        Args:
            max_age_hours: Supprime les caches plus vieux que ce nombre d'heures
        """
        try:
            cursor = self.conn.cursor()
            cutoff = time.time() - (max_age_hours * 3600)

            cursor.execute('DELETE FROM weather_cache WHERE timestamp < ?', (cutoff,))
            deleted = cursor.rowcount
            self.conn.commit()

            if deleted > 0:
                logger.info(f"Nettoyage cache m√©t√©o : {deleted} entr√©es supprim√©es (> {max_age_hours}h)")

        except Exception as e:
            logger.error(f"Erreur lors du nettoyage du cache m√©t√©o : {e}")

    def export_neighbors_to_json(self, hours: int = 48):
        """
        Exporter les donn√©es de voisinage au format JSON pour les cartes HTML.
        Compatible avec le format attendu par map/export_neighbors.py
        
        Args:
            hours: Nombre d'heures de donn√©es √† exporter
            
        Returns:
            dict: Structure JSON avec les informations de voisinage
        """
        try:
            from datetime import datetime
            import json
            
            neighbors_data = self.load_neighbors(hours=hours)
            
            # Structure de sortie compatible avec export_neighbors.py
            output_data = {
                'export_time': datetime.now().isoformat(),
                'source': 'meshbot_database',
                'total_nodes': len(neighbors_data),
                'nodes': {}
            }
            
            # Convertir les donn√©es pour chaque n≈ìud
            for node_id, neighbors in neighbors_data.items():
                # Convertir node_id en format !xxxxxxxx si n√©cessaire
                if not node_id.startswith('!'):
                    node_id_formatted = f"!{node_id}"
                else:
                    node_id_formatted = node_id
                
                output_data['nodes'][node_id_formatted] = {
                    'neighbors_extracted': neighbors,
                    'neighbor_count': len(neighbors),
                    'export_timestamp': datetime.now().isoformat()
                }
            
            # Statistiques
            total_neighbors = sum(len(n) for n in neighbors_data.values())
            nodes_with_neighbors = len([n for n in neighbors_data.values() if len(n) > 0])
            
            output_data['statistics'] = {
                'nodes_with_neighbors': nodes_with_neighbors,
                'total_neighbor_entries': total_neighbors,
                'average_neighbors': total_neighbors / len(neighbors_data) if neighbors_data else 0
            }
            
            return output_data
            
        except Exception as e:
            logger.error(f"Erreur lors de l'export JSON des voisins : {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {}

    def get_node_position_from_db(self, node_id: str, hours: int = 720) -> Optional[Dict]:
        """
        R√©cup√®re la derni√®re position GPS connue d'un n≈ìud depuis la base de donn√©es.
        
        Args:
            node_id: ID du n≈ìud (peut √™tre int, string d√©cimal, ou hex avec !)
            hours: Nombre d'heures √† chercher en arri√®re (d√©faut: 720h = 30 jours)
            
        Returns:
            Dict avec 'latitude', 'longitude' et 'altitude' ou None si pas trouv√©
        """
        try:
            cursor = self.conn.cursor()
            cutoff = (datetime.now() - timedelta(hours=hours)).timestamp()
            
            # Convertir node_id en diff√©rents formats pour maximiser les chances de match
            search_ids = []
            
            # Si c'est un entier ou une cha√Æne num√©rique
            try:
                if isinstance(node_id, str) and node_id.startswith('!'):
                    # Format !hex
                    node_id_int = int(node_id[1:], 16)
                    search_ids = [node_id, str(node_id_int), node_id_int]
                elif isinstance(node_id, (int, str)):
                    node_id_int = int(node_id) if isinstance(node_id, str) else node_id
                    node_id_hex = f"!{node_id_int:08x}"
                    search_ids = [str(node_id_int), node_id_int, node_id_hex]
            except (ValueError, AttributeError):
                search_ids = [node_id]
            
            debug_print(f"get_node_position_from_db: node_id={node_id}, search_ids={search_ids}, hours={hours}")
            
            # Essayer de trouver une position avec n'importe quel format d'ID
            for search_id in search_ids:
                cursor.execute('''
                    SELECT position
                    FROM packets
                    WHERE from_id = ?
                        AND timestamp >= ?
                        AND position IS NOT NULL
                        AND position != ''
                    ORDER BY timestamp DESC
                    LIMIT 1
                ''', (search_id, cutoff))
                
                row = cursor.fetchone()
                if row and row['position']:
                    try:
                        position = json.loads(row['position'])
                        lat = position.get('latitude')
                        lon = position.get('longitude')
                        alt = position.get('altitude')
                        if lat and lon and lat != 0 and lon != 0:
                            debug_print(f"‚úÖ Position trouv√©e pour {node_id} (via {search_id}): lat={lat}, lon={lon}, alt={alt}")
                            return {'latitude': lat, 'longitude': lon, 'altitude': alt}
                        else:
                            debug_print(f"‚ö†Ô∏è Position invalide pour {node_id}: lat={lat}, lon={lon}")
                    except (json.JSONDecodeError, KeyError, TypeError) as e:
                        debug_print(f"‚ùå Erreur parsing position pour {node_id}: {e}")
                        continue
            
            debug_print(f"‚ùå Aucune position trouv√©e pour {node_id} (cherch√©: {search_ids})")
            return None
            
        except Exception as e:
            error_print(f"Erreur lors de la r√©cup√©ration de la position du n≈ìud {node_id} : {e}")
            return None

    def load_radio_links_with_positions(self, hours: int = 24) -> List[Dict]:
        """
        Charge les liaisons radio avec les positions GPS pour calculer les distances.
        
        Args:
            hours: Nombre d'heures √† charger
            
        Returns:
            Liste de dicts avec from_id, to_id, snr, rssi, timestamp, from_lat, from_lon, to_lat, to_lon
        """
        try:
            cursor = self.conn.cursor()
            cutoff = (datetime.now() - timedelta(hours=hours)).timestamp()
            
            # Requ√™te pour r√©cup√©rer les paquets avec positions GPS quand disponibles
            # On utilise les donn√©es de position stock√©es dans le JSON position
            cursor.execute('''
                SELECT 
                    from_id, 
                    to_id, 
                    snr, 
                    rssi,
                    timestamp,
                    position as position_json
                FROM packets
                WHERE timestamp >= ?
                    AND from_id IS NOT NULL 
                    AND to_id IS NOT NULL
                    AND to_id != 4294967295
                    AND to_id != 0
                    AND from_id != to_id
                    AND (snr IS NOT NULL OR rssi IS NOT NULL)
                ORDER BY timestamp DESC
            ''', (cutoff,))
            
            links = []
            for row in cursor.fetchall():
                link = {
                    'from_id': row['from_id'],
                    'to_id': row['to_id'],
                    'snr': row['snr'],
                    'rssi': row['rssi'],
                    'timestamp': row['timestamp']
                }
                
                # Parser le JSON de position si pr√©sent (position de l'√©metteur)
                if row['position_json']:
                    try:
                        position = json.loads(row['position_json'])
                        link['sender_lat'] = position.get('latitude')
                        link['sender_lon'] = position.get('longitude')
                    except (json.JSONDecodeError, KeyError):
                        pass
                
                links.append(link)
            
            return links
            
        except Exception as e:
            logger.error(f"Erreur lors du chargement des liaisons radio : {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def close(self):
        """Ferme la connexion √† la base de donn√©es."""
        if self.conn:
            self.conn.close()
            logger.info("Connexion √† la base de donn√©es ferm√©e")

    def __del__(self):
        """Destructeur pour s'assurer que la connexion est ferm√©e."""
        self.close()
